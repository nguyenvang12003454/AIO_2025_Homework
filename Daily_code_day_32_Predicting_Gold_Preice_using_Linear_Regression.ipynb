{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9BAqG2yW0PTrlAMQL1w2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenvang12003454/AIO_2025_Homework/blob/main/Daily_code_day_32_Predicting_Gold_Preice_using_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Gold Price using Linear Regression"
      ],
      "metadata": {
        "id": "QAkpmxZLWpsL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load dataset"
      ],
      "metadata": {
        "id": "MLy47akAXS8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lsRTbw6BWgth"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "X = np.array([\n",
        "    [92.5, 2.1, 65.3],\n",
        "    [93.2, 2.5, 67.2],\n",
        "    [91.8, 2.3, 64.0],\n",
        "    [94.0, 2.8, 70.1],\n",
        "    [95.2, 3.0, 72.5],\n",
        "    [96.1, 3.2, 74.3],\n",
        "    [90.5, 1.8, 61.0],\n",
        "    [92.0, 2.0, 63.2],\n",
        "    [89.5, 1.5, 59.8],\n",
        "    [97.0, 3.5, 76.2],\n",
        "    [95.8, 3.1, 73.8],\n",
        "    [94.5, 2.9, 71.5],\n",
        "    [91.2, 2.2, 62.8],\n",
        "    [90.0, 1.7, 60.5],\n",
        "    [98.0, 3.7, 78.0],\n",
        "    [99.2, 4.0, 80.5],\n",
        "    [88.5, 1.3, 58.0],\n",
        "    [87.8, 1.1, 56.5],\n",
        "    [86.5, 1.0, 55.0],\n",
        "    [100.0, 4.2, 82.0]\n",
        "])\n",
        "\n",
        "y = np.array([\n",
        "    1800, 1825, 1795, 1850, 1880, 1905, 1750, 1780, 1725, 1925,\n",
        "    1890, 1860, 1775, 1740, 1950, 1980, 1700, 1680, 1650, 2000\n",
        "])"
      ],
      "metadata": {
        "id": "MI5Jube2XhlV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Linear Regression Model"
      ],
      "metadata": {
        "id": "kBDXrIsoXqC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self):\n",
        "    self.w = np.zeros(X.shape[1])\n",
        "    self.b = 0\n",
        "\n",
        "  def compute_gradient(self, X, y):\n",
        "    m = len(y)\n",
        "\n",
        "    y_pred = np.dot(X, self.w) + self.b\n",
        "    dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
        "    db = (1/m) * np.sum(y_pred - y)\n",
        "\n",
        "    return dw, db\n",
        "\n",
        "  def fit(self, X, y, learning_rate=0.001, epochs = 10):\n",
        "    for epoch in range(epochs):\n",
        "      dw, db = self.compute_gradient(X, y)\n",
        "      self.w -= learning_rate * dw\n",
        "      self.b -= learning_rate * db\n",
        "\n",
        "      y_pred = np.dot(X, self.w) + self.b\n",
        "      loss = np.mean((y_pred - y)**2)\n",
        "\n",
        "      print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.dot(X, self.w) + self.b"
      ],
      "metadata": {
        "id": "3waarK_rXutN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize features X\n",
        "X_norm = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# Initialize and Training model\n",
        "model = LinearRegression()\n",
        "model.fit(X_norm, y, learning_rate=0.01, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wPlAoKmWYE7L",
        "outputId": "1db1a3d1-cdd5-49e0-f259-900a1ea44388"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 3266224.1247206153\n",
            "Epoch 2/100, Loss: 3200875.039518687\n",
            "Epoch 3/100, Loss: 3136847.098501115\n",
            "Epoch 4/100, Loss: 3074112.7921351637\n",
            "Epoch 5/100, Loss: 3012645.230094502\n",
            "Epoch 6/100, Loss: 2952418.1247110227\n",
            "Epoch 7/100, Loss: 2893405.775004822\n",
            "Epoch 8/100, Loss: 2835583.051266168\n",
            "Epoch 9/100, Loss: 2778925.380164693\n",
            "Epoch 10/100, Loss: 2723408.73036234\n",
            "Epoch 11/100, Loss: 2669009.5986078093\n",
            "Epoch 12/100, Loss: 2615704.9962914623\n",
            "Epoch 13/100, Loss: 2563472.4364406513\n",
            "Epoch 14/100, Loss: 2512289.921136584\n",
            "Epoch 15/100, Loss: 2462135.9293347085\n",
            "Epoch 16/100, Loss: 2412989.4050715985\n",
            "Epoch 17/100, Loss: 2364829.74604215\n",
            "Epoch 18/100, Loss: 2317636.7925317367\n",
            "Epoch 19/100, Loss: 2271390.816688738\n",
            "Epoch 20/100, Loss: 2226072.5121236076\n",
            "Epoch 21/100, Loss: 2181662.9838213087\n",
            "Epoch 22/100, Loss: 2138143.7383546275\n",
            "Epoch 23/100, Loss: 2095496.6743864748\n",
            "Epoch 24/100, Loss: 2053704.073449881\n",
            "Epoch 25/100, Loss: 2012748.5909949348\n",
            "Epoch 26/100, Loss: 1972613.247692429\n",
            "Epoch 27/100, Loss: 1933281.420984496\n",
            "Epoch 28/100, Loss: 1894736.8368729558\n",
            "Epoch 29/100, Loss: 1856963.5619365454\n",
            "Epoch 30/100, Loss: 1819945.9955686275\n",
            "Epoch 31/100, Loss: 1783668.8624273606\n",
            "Epoch 32/100, Loss: 1748117.2050906823\n",
            "Epoch 33/100, Loss: 1713276.3769088243\n",
            "Epoch 34/100, Loss: 1679132.0350473993\n",
            "Epoch 35/100, Loss: 1645670.1337144168\n",
            "Epoch 36/100, Loss: 1612876.9175649027\n",
            "Epoch 37/100, Loss: 1580738.9152770496\n",
            "Epoch 38/100, Loss: 1549242.9332941275\n",
            "Epoch 39/100, Loss: 1518376.0497266115\n",
            "Epoch 40/100, Loss: 1488125.6084092436\n",
            "Epoch 41/100, Loss: 1458479.213107969\n",
            "Epoch 42/100, Loss: 1429424.721871894\n",
            "Epoch 43/100, Loss: 1400950.2415256414\n",
            "Epoch 44/100, Loss: 1373044.122297647\n",
            "Epoch 45/100, Loss: 1345694.952580153\n",
            "Epoch 46/100, Loss: 1318891.5538168089\n",
            "Epoch 47/100, Loss: 1292622.975513971\n",
            "Epoch 48/100, Loss: 1266878.4903719425\n",
            "Epoch 49/100, Loss: 1241647.5895325565\n",
            "Epoch 50/100, Loss: 1216919.97793963\n",
            "Epoch 51/100, Loss: 1192685.5698089781\n",
            "Epoch 52/100, Loss: 1168934.484204788\n",
            "Epoch 53/100, Loss: 1145657.040719273\n",
            "Epoch 54/100, Loss: 1122843.7552526721\n",
            "Epoch 55/100, Loss: 1100485.3358907332\n",
            "Epoch 56/100, Loss: 1078572.6788769541\n",
            "Epoch 57/100, Loss: 1057096.864676952\n",
            "Epoch 58/100, Loss: 1036049.1541324034\n",
            "Epoch 59/100, Loss: 1015420.984702137\n",
            "Epoch 60/100, Loss: 995203.9667879923\n",
            "Epoch 61/100, Loss: 975389.8801431922\n",
            "Epoch 62/100, Loss: 955970.6703610218\n",
            "Epoch 63/100, Loss: 936938.445441698\n",
            "Epoch 64/100, Loss: 918285.4724353874\n",
            "Epoch 65/100, Loss: 900004.1741593916\n",
            "Epoch 66/100, Loss: 882087.1259875937\n",
            "Epoch 67/100, Loss: 864527.052710318\n",
            "Epoch 68/100, Loss: 847316.8254628179\n",
            "Epoch 69/100, Loss: 830449.4587206664\n",
            "Epoch 70/100, Loss: 813918.107360373\n",
            "Epoch 71/100, Loss: 797716.063783616\n",
            "Epoch 72/100, Loss: 781836.7551035151\n",
            "Epoch 73/100, Loss: 766273.7403914324\n",
            "Epoch 74/100, Loss: 751020.7079828267\n",
            "Epoch 75/100, Loss: 736071.4728407365\n",
            "Epoch 76/100, Loss: 721419.9739755063\n",
            "Epoch 77/100, Loss: 707060.2719194175\n",
            "Epoch 78/100, Loss: 692986.5462549216\n",
            "Epoch 79/100, Loss: 679193.0931952072\n",
            "Epoch 80/100, Loss: 665674.3232158805\n",
            "Epoch 81/100, Loss: 652424.7587365641\n",
            "Epoch 82/100, Loss: 639439.0318512571\n",
            "Epoch 83/100, Loss: 626711.8821063298\n",
            "Epoch 84/100, Loss: 614238.1543250638\n",
            "Epoch 85/100, Loss: 602012.7964776701\n",
            "Epoch 86/100, Loss: 590030.8575957547\n",
            "Epoch 87/100, Loss: 578287.4857302217\n",
            "Epoch 88/100, Loss: 566777.9259516412\n",
            "Epoch 89/100, Loss: 555497.5183921237\n",
            "Epoch 90/100, Loss: 544441.6963277783\n",
            "Epoch 91/100, Loss: 533605.9843008493\n",
            "Epoch 92/100, Loss: 522985.9962806536\n",
            "Epoch 93/100, Loss: 512577.4338624614\n",
            "Epoch 94/100, Loss: 502376.08450348675\n",
            "Epoch 95/100, Loss: 492377.8197951747\n",
            "Epoch 96/100, Loss: 482578.5937709921\n",
            "Epoch 97/100, Loss: 472974.4412489487\n",
            "Epoch 98/100, Loss: 463561.47620809916\n",
            "Epoch 99/100, Loss: 454335.8901982828\n",
            "Epoch 100/100, Loss: 445293.95078239695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print final weights and bias\n",
        "print(f'Final weights: {model.w}')\n",
        "print(f'Final bias: {model.b}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7YHJ4URZI2G",
        "outputId": "5d206feb-1d61-45be-a720-c134e566ac5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights: [31.26700668 31.06183274 31.0143614 ]\n",
            "Final bias: 1155.7230418589024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict and compare results\n",
        "y_pred = model.predict(X_norm)\n",
        "\n",
        "print('Compare real and predicted values:')\n",
        "for i in range(len(y)):\n",
        "  print(f'Real: {y[i]:.2f}, Predicted: {y_pred[i]:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjHoRckTZYNH",
        "outputId": "667cefaf-e57f-4536-938e-dc9ca03bc2b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compare real and predicted values:\n",
            "Real: 1800.00, Predicted: 1127.94\n",
            "Real: 1825.00, Predicted: 1154.59\n",
            "Real: 1795.00, Predicted: 1123.70\n",
            "Real: 1850.00, Predicted: 1182.62\n",
            "Real: 1880.00, Predicted: 1208.73\n",
            "Real: 1905.00, Predicted: 1229.98\n",
            "Real: 1750.00, Predicted: 1084.38\n",
            "Real: 1780.00, Predicted: 1112.24\n",
            "Real: 1725.00, Predicted: 1061.28\n",
            "Real: 1925.00, Predicted: 1254.96\n",
            "Real: 1890.00, Predicted: 1222.17\n",
            "Real: 1860.00, Predicted: 1195.61\n",
            "Real: 1775.00, Predicted: 1110.64\n",
            "Real: 1740.00, Predicted: 1074.89\n",
            "Real: 1950.00, Predicted: 1277.06\n",
            "Real: 1980.00, Predicted: 1306.90\n",
            "Real: 1700.00, Predicted: 1039.18\n",
            "Real: 1680.00, Predicted: 1020.78\n",
            "Real: 1650.00, Predicted: 1000.67\n",
            "Real: 2000.00, Predicted: 1326.14\n"
          ]
        }
      ]
    }
  ]
}